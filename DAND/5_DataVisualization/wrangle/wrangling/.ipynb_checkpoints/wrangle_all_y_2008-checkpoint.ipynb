{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl  # for formatting xticks into thousands\n",
    "import operator # for sorting dict. NOTE: becomes list of tuples\n",
    "from tqdm import *\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import datetime # redundant? \n",
    "import plotly.plotly as py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to login to plotpl api key\n",
    "import plotly as py1\n",
    "py1.tools.set_credentials_file(username='CleanData',api_key='tt3PmRCW8uc3FowXCDZ7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_my_palette():\n",
    "    sb.set()\n",
    "    current_palette = sb.color_palette(my_palette)\n",
    "    sb.set_palette(current_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose color https://www.w3schools.com/colors/colors_picker.asp\n",
    "my_palette = ['#4da6ff', '#00b386', '#ff6666', '#ffff66', '#8c66ff', '#4dffd2']\n",
    "set_my_palette()\n",
    "sb.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sb.color_palette()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_number(x):\n",
    "     return \"{:,.0f}\".format(x)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_full_df = pd.read_csv('../raw_data/2008/2008.csv')\n",
    "carriers_df = pd.read_csv('../raw_data/supplement/carriers.csv')\n",
    "airports_df = pd.read_csv('../raw_data/supplement/airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_2008_df = y_2008_full_df.sample(frac=0.1,replace=False)\n",
    "#y_2008_df = y_2008_full_df.sample(frac=0.025,replace=False)\n",
    "#y_2008_df = y_2008_full_df.sample(frac=0.01,replace=False)\n",
    "y_2008_df = y_2008_full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href='https://docs.google.com/document/d/e/2PACX-1vQmkX4iOT6Rcrin42vslquX2_wQCjIa_hbwD0xmxrERPSOJYDtpNc_3wwK_p9_KpOsfA6QVyEHdxxq7/pub?embedded=True'>Link to data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>...</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>CancellationCode</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>CarrierDelay</th>\n",
       "      <th>WeatherDelay</th>\n",
       "      <th>NASDelay</th>\n",
       "      <th>SecurityDelay</th>\n",
       "      <th>LateAircraftDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2225</td>\n",
       "      <td>WN</td>\n",
       "      <td>335</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>754.0</td>\n",
       "      <td>735</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>WN</td>\n",
       "      <td>3231</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
       "0  2008      1           3          4   2003.0        1955   2211.0   \n",
       "1  2008      1           3          4    754.0         735   1002.0   \n",
       "\n",
       "   CRSArrTime UniqueCarrier  FlightNum        ...         TaxiIn  TaxiOut  \\\n",
       "0        2225            WN        335        ...            4.0      8.0   \n",
       "1        1000            WN       3231        ...            5.0     10.0   \n",
       "\n",
       "   Cancelled  CancellationCode  Diverted  CarrierDelay WeatherDelay NASDelay  \\\n",
       "0          0               NaN         0           NaN          NaN      NaN   \n",
       "1          0               NaN         0           NaN          NaN      NaN   \n",
       "\n",
       "   SecurityDelay  LateAircraftDelay  \n",
       "0            NaN                NaN  \n",
       "1            NaN                NaN  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>...</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>CancellationCode</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>CarrierDelay</th>\n",
       "      <th>WeatherDelay</th>\n",
       "      <th>NASDelay</th>\n",
       "      <th>SecurityDelay</th>\n",
       "      <th>LateAircraftDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2225</td>\n",
       "      <td>WN</td>\n",
       "      <td>335</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>754.0</td>\n",
       "      <td>735</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>WN</td>\n",
       "      <td>3231</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
       "0  2008      1           3          4   2003.0        1955   2211.0   \n",
       "1  2008      1           3          4    754.0         735   1002.0   \n",
       "\n",
       "   CRSArrTime UniqueCarrier  FlightNum        ...         TaxiIn  TaxiOut  \\\n",
       "0        2225            WN        335        ...            4.0      8.0   \n",
       "1        1000            WN       3231        ...            5.0     10.0   \n",
       "\n",
       "   Cancelled  CancellationCode  Diverted  CarrierDelay WeatherDelay NASDelay  \\\n",
       "0          0               NaN         0           NaN          NaN      NaN   \n",
       "1          0               NaN         0           NaN          NaN      NaN   \n",
       "\n",
       "   SecurityDelay  LateAircraftDelay  \n",
       "0            NaN                NaN  \n",
       "1            NaN                NaN  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df = y_2008_df.reset_index().drop('index',axis=1)\n",
    "y_2008_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7009728"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7009728 entries, 0 to 7009727\n",
      "Data columns (total 29 columns):\n",
      "Year                 int64\n",
      "Month                int64\n",
      "DayofMonth           int64\n",
      "DayOfWeek            int64\n",
      "DepTime              float64\n",
      "CRSDepTime           int64\n",
      "ArrTime              float64\n",
      "CRSArrTime           int64\n",
      "UniqueCarrier        object\n",
      "FlightNum            int64\n",
      "TailNum              object\n",
      "ActualElapsedTime    float64\n",
      "CRSElapsedTime       float64\n",
      "AirTime              float64\n",
      "ArrDelay             float64\n",
      "DepDelay             float64\n",
      "Origin               object\n",
      "Dest                 object\n",
      "Distance             int64\n",
      "TaxiIn               float64\n",
      "TaxiOut              float64\n",
      "Cancelled            int64\n",
      "CancellationCode     object\n",
      "Diverted             int64\n",
      "CarrierDelay         float64\n",
      "WeatherDelay         float64\n",
      "NASDelay             float64\n",
      "SecurityDelay        float64\n",
      "LateAircraftDelay    float64\n",
      "dtypes: float64(14), int64(10), object(5)\n",
      "memory usage: 1.5+ GB\n"
     ]
    }
   ],
   "source": [
    "y_2008_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating DepDelay and ArrDelay\n",
    "- Because Delays are one of our main topic of interests, I would begin by investigating DepDelay and ArrDelay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df.DepDelay.dtypes, y_2008_df.ArrDelay.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Both are floats. So can perform addition and subtraction on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DepTime','CRSDepTime','DepDelay','ArrTime','CRSArrTime','ArrDelay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2225</td>\n",
       "      <td>-14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>754.0</td>\n",
       "      <td>735</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>628.0</td>\n",
       "      <td>620</td>\n",
       "      <td>8.0</td>\n",
       "      <td>804.0</td>\n",
       "      <td>750</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>926.0</td>\n",
       "      <td>930</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>1100</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1829.0</td>\n",
       "      <td>1755</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>1925</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1940.0</td>\n",
       "      <td>1915</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2121.0</td>\n",
       "      <td>2110</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1937.0</td>\n",
       "      <td>1830</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2037.0</td>\n",
       "      <td>1940</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1039.0</td>\n",
       "      <td>1040</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>1150</td>\n",
       "      <td>-18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>617.0</td>\n",
       "      <td>615</td>\n",
       "      <td>2.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>650</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1639.0</td>\n",
       "      <td>1655</td>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DepTime  CRSDepTime  DepDelay  ArrTime  CRSArrTime  ArrDelay\n",
       "0   2003.0        1955       8.0   2211.0        2225     -14.0\n",
       "1    754.0         735      19.0   1002.0        1000       2.0\n",
       "2    628.0         620       8.0    804.0         750      14.0\n",
       "3    926.0         930      -4.0   1054.0        1100      -6.0\n",
       "4   1829.0        1755      34.0   1959.0        1925      34.0\n",
       "5   1940.0        1915      25.0   2121.0        2110      11.0\n",
       "6   1937.0        1830      67.0   2037.0        1940      57.0\n",
       "7   1039.0        1040      -1.0   1132.0        1150     -18.0\n",
       "8    617.0         615       2.0    652.0         650       2.0\n",
       "9   1620.0        1620       0.0   1639.0        1655     -16.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df[cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     48.0\n",
       "1     19.0\n",
       "2      8.0\n",
       "3     -4.0\n",
       "4     74.0\n",
       "5     25.0\n",
       "6    107.0\n",
       "7     -1.0\n",
       "8      2.0\n",
       "9      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df.DepTime[:10] - y_2008_df.CRSDepTime[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -14.0\n",
       "1     2.0\n",
       "2    54.0\n",
       "3   -46.0\n",
       "4    34.0\n",
       "5    11.0\n",
       "6    97.0\n",
       "7   -18.0\n",
       "8     2.0\n",
       "9   -16.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df.ArrTime[:10] - y_2008_df.CRSArrTime[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare with above table. There are errors. So I have to manuually calculate DepDelay and ArrDelay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert into DateTime Object\n",
    "https://towardsdatascience.com/basic-time-series-manipulation-with-pandas-4432afee64ea\n",
    "\n",
    "- To calculate DepDelay and ArrDelay manually, we need to reformat our data into datetime object. Without reformatting, it would be downright difficult to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Combine Year, Month and DayofMonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                   int64\n",
       "Month                  int64\n",
       "DayofMonth             int64\n",
       "DayOfWeek              int64\n",
       "DepTime              float64\n",
       "CRSDepTime             int64\n",
       "ArrTime              float64\n",
       "CRSArrTime             int64\n",
       "UniqueCarrier         object\n",
       "FlightNum              int64\n",
       "TailNum               object\n",
       "ActualElapsedTime    float64\n",
       "CRSElapsedTime       float64\n",
       "AirTime              float64\n",
       "ArrDelay             float64\n",
       "DepDelay             float64\n",
       "Origin                object\n",
       "Dest                  object\n",
       "Distance               int64\n",
       "TaxiIn               float64\n",
       "TaxiOut              float64\n",
       "Cancelled              int64\n",
       "CancellationCode      object\n",
       "Diverted               int64\n",
       "CarrierDelay         float64\n",
       "WeatherDelay         float64\n",
       "NASDelay             float64\n",
       "SecurityDelay        float64\n",
       "LateAircraftDelay    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>...</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>CancellationCode</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>CarrierDelay</th>\n",
       "      <th>WeatherDelay</th>\n",
       "      <th>NASDelay</th>\n",
       "      <th>SecurityDelay</th>\n",
       "      <th>LateAircraftDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2225</td>\n",
       "      <td>WN</td>\n",
       "      <td>335</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>754.0</td>\n",
       "      <td>735</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>WN</td>\n",
       "      <td>3231</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
       "0  2008      1           3          4   2003.0        1955   2211.0   \n",
       "1  2008      1           3          4    754.0         735   1002.0   \n",
       "\n",
       "   CRSArrTime UniqueCarrier  FlightNum        ...         TaxiIn  TaxiOut  \\\n",
       "0        2225            WN        335        ...            4.0      8.0   \n",
       "1        1000            WN       3231        ...            5.0     10.0   \n",
       "\n",
       "   Cancelled  CancellationCode  Diverted  CarrierDelay WeatherDelay NASDelay  \\\n",
       "0          0               NaN         0           NaN          NaN      NaN   \n",
       "1          0               NaN         0           NaN          NaN      NaN   \n",
       "\n",
       "   SecurityDelay  LateAircraftDelay  \n",
       "0            NaN                NaN  \n",
       "1            NaN                NaN  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2008_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to combine columns Year, Month and DayofMonth.\n",
    "- For example: 2008-05-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Year = y_2008_df.Year.astype('str')\n",
    "y_2008_df.Month = y_2008_df.Month.astype('str')\n",
    "y_2008_df.DayofMonth = y_2008_df.DayofMonth.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_date = y_2008_df.Year + \"-\" + y_2008_df.Month + \"-\" + y_2008_df.DayofMonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then make it into datetime object\n",
    "y_2008_df['CombinedDate'] = pd.to_datetime(combined_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_test = ['Year','Month','DayofMonth','CombinedDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[cols_to_test].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CombinedDate.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great! All converted into datetime object. And there is no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Convert DepTime into DateTime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[['CombinedDate','DepTime']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- We want to convert DepTime into DateTime object.\n",
    "- For example: 2008-05-02 05:54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into string\n",
    "y_2008_df.DepTime = y_2008_df.DepTime.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.DepTime.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string by a period.  get the first element\n",
    "y_2008_df.DepTime.str.split('.').apply(lambda x: x[0]).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace = True\n",
    "y_2008_df.DepTime = y_2008_df.DepTime.str.split('.').apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_leading_zeros(element):\n",
    "    numStr=element\n",
    "    if numStr=='nan':\n",
    "        numStr = np.NaN\n",
    "    else:\n",
    "        numStr = element.rjust(4,'0')\n",
    "    #print(numStr)\n",
    "    return numStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add leading zeros\n",
    "y_2008_df.DepTime = y_2008_df.DepTime.apply(add_leading_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.DepTime.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_colon(element):\n",
    "    element = str(element)\n",
    "    numStr = element\n",
    "    if element == 'nan':\n",
    "        numStr=np.NaN\n",
    "    else:\n",
    "        element = str(element)\n",
    "        numStr = element[:2]+':'+element[2:]\n",
    "    return numStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.DepTime = y_2008_df.DepTime.apply(add_colon)\n",
    "y_2008_df.DepTime.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine CombineDateTime and DepTime and make it a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = []\n",
    "for indx,val in enumerate(y_2008_df.DepTime):\n",
    "    #print(val)\n",
    "    if val == 0:\n",
    "        #print('null')\n",
    "        date_list.append(pd.NaT)  # somehow for datetime NaT is equivalent to NaN\n",
    "    else:\n",
    "        date_list.append( y_2008_df.loc[indx,'Year'] + \"-\" + y_2008_df.loc[indx,'Month']\\\n",
    "                 + \"-\" + y_2008_df.loc[indx,'DayofMonth'] + \" \" + str(val))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df['DepTime'] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into DateTime object\n",
    "y_2008_df['DepTime']=pd.to_datetime(y_2008_df.DepTime,errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.DepTime.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Convert ArrTime into DateTime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[['CombinedDate','ArrTime']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- We want to convert ArrTime into DateTime object.\n",
    "- For example: 2008-08-17 14:08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into string\n",
    "y_2008_df.ArrTime = y_2008_df.ArrTime.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.ArrTime.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string by a period.  get the first element\n",
    "y_2008_df.ArrTime.str.split('.').apply(lambda x: x[0]).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace = True\n",
    "y_2008_df.ArrTime = y_2008_df.ArrTime.str.split('.').apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add leading zeros\n",
    "y_2008_df.ArrTime = y_2008_df.ArrTime.apply(add_leading_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.ArrTime.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.ArrTime = y_2008_df.ArrTime.apply(add_colon)\n",
    "y_2008_df.ArrTime.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine CombineDateTime and ArrTime and make it a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = []\n",
    "for indx,val in enumerate(y_2008_df.ArrTime):\n",
    "    #print(val)\n",
    "    if val == 0:\n",
    "        #print('null')\n",
    "        date_list.append(pd.NaT)  # somehow for datetime NaT is equivalent to NaN\n",
    "    else:\n",
    "        date_list.append( y_2008_df.loc[indx,'Year'] + \"-\" + y_2008_df.loc[indx,'Month']\\\n",
    "                 + \"-\" + y_2008_df.loc[indx,'DayofMonth'] + \" \" + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df['ArrTime'] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into DateTime object\n",
    "y_2008_df['ArrTime']=pd.to_datetime(y_2008_df.ArrTime,errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.ArrTime.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Convert CRSDepTime into DateTime object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- We want to convert CRSDepTime into DateTime object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into string\n",
    "y_2008_df.CRSDepTime = y_2008_df.CRSDepTime.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace = True\n",
    "y_2008_df.CRSDepTime = y_2008_df.CRSDepTime.str.split('.').apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add leading zeros\n",
    "y_2008_df.CRSDepTime = y_2008_df.CRSDepTime.apply(add_leading_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CRSDepTime = y_2008_df.CRSDepTime.apply(add_colon)\n",
    "y_2008_df.CRSDepTime.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine CombineDateTime and CRSDepTime and make it a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = []\n",
    "for indx,val in enumerate(y_2008_df.CRSDepTime):\n",
    "    #print(val)\n",
    "    if val == 0:\n",
    "        #print('null')\n",
    "        date_list.append(pd.NaT)  # somehow for datetime NaT is equivalent to NaN\n",
    "    else:\n",
    "        date_list.append( y_2008_df.loc[indx,'Year'] + \"-\" + y_2008_df.loc[indx,'Month']\\\n",
    "                 + \"-\" + y_2008_df.loc[indx,'DayofMonth'] + \" \" + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df['CRSDepTime'] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into DateTime object\n",
    "y_2008_df['CRSDepTime']=pd.to_datetime(y_2008_df.CRSDepTime,errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CRSDepTime.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Convert CRSArrTime into DateTime object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- We want to convert CRSArrTime into DateTime object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into string\n",
    "y_2008_df.CRSArrTime = y_2008_df.CRSArrTime.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace = True\n",
    "y_2008_df.CRSArrTime = y_2008_df.CRSArrTime.str.split('.').apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add leading zeros\n",
    "y_2008_df.CRSArrTime = y_2008_df.CRSArrTime.apply(add_leading_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CRSArrTime = y_2008_df.CRSArrTime.apply(add_colon)\n",
    "y_2008_df.CRSArrTime.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine CombineDateTime and CRSArrTime and make it a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = []\n",
    "for indx,val in enumerate(y_2008_df.CRSArrTime):\n",
    "    #print(val)\n",
    "    if val == 0:\n",
    "        #print('null')\n",
    "        date_list.append(pd.NaT)  # somehow for datetime NaT is equivalent to NaN\n",
    "    else:\n",
    "        date_list.append( y_2008_df.loc[indx,'Year'] + \"-\" + y_2008_df.loc[indx,'Month']\\\n",
    "                 + \"-\" + y_2008_df.loc[indx,'DayofMonth'] + \" \" + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df['CRSArrTime'] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into DateTime object\n",
    "y_2008_df['CRSArrTime']=pd.to_datetime(y_2008_df.CRSArrTime,errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CRSArrTime.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Manually Calculating DepDelay and ArrDelay\n",
    "- Now that we have converted necessary columns into DateTime object, it's time to manually calculuate DepDelay and ArrDelay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return in minutes between two dates -- d1 - d2\n",
    "def days_between(d1,d2):\n",
    "    if (pd.isnull(d1)) | (pd.isnull(d2)):\n",
    "        #print('null')\n",
    "        return np.NaN\n",
    "    return ((d1 - d2).total_seconds())/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DepDelay\n",
    "#### Define\n",
    "- Calculate manually DepDelay by subtracting DepTime from CRSDepTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.DepTime[0],y_2008_df.CRSDepTime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_between(y_2008_df.DepTime[0],y_2008_df.CRSDepTime[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inspect the above result. Is the answer correct?\n",
    "- Now do this for all rows !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate DepDelays for all rows manually\n",
    "diff_list = []\n",
    "for val1,val2 in zip(y_2008_df.DepTime,y_2008_df.CRSDepTime):\n",
    "    diff_list.append(days_between(val1,val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'DepDelayM' and store the new calculated Delays\n",
    "y_2008_df['DepDelayM']=diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DepTime','CRSDepTime','DepDelayM','DepDelay']\n",
    "y_2008_df[cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mostly identical DepDelay and DepDelayM columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now what is the percentage of identical DepDelayM and DepDelay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelay == y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())][cols].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelay == y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())].shape[0], y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Around 95% is identical. \n",
    "- Therefore we still need to investigate the differences in columns between DepDelay and DepDelayM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the percentage of non-identical DepDelayM and DepDelay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelay != y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())].shape[0], y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.3% where DepDelayM and DepDelay is not identical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelay != y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())][cols].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looking at the table above, we can see that CRSDepTime is scheduled at night but the DepTime happened a day before CRSDepTime. Therefore, to fix this error, we need to forward DepTime by one day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Fixing when DepDelayM and DepDelay is not identical (This consists of only approximately 0.3% of our total data!)\n",
    "- This is done by forwarding DepTime by one day. Then we need to re-calculate DepDelayM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_forward_one = y_2008_df[(y_2008_df.DepDelay != y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())][cols].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forwward DepTime within index by oneday\n",
    "for indx in indx_forward_one:\n",
    "    y_2008_df.loc[indx,'DepTime'] += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate Delay in mins\n",
    "for indx in indx_forward_one:\n",
    "    val1=y_2008_df.loc[indx,'DepTime']\n",
    "    val2=y_2008_df.loc[indx,'CRSDepTime']\n",
    "    diff_min = days_between(val1,val2)\n",
    "    y_2008_df.loc[indx,'DepDelayM'] = diff_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelay != y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())].shape[0], y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelay != y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So, I managed to reduce to a few rows. (depends on random sample) I won't recalculate DepDelayM anymore since this consists of only a miniscule of our data. The best action to take now is to drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_to_del = y_2008_df[(y_2008_df.DepDelay != y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())][cols].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.drop(index=indx_to_del,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df = y_2008_df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelay != y_2008_df.DepDelayM)\\\n",
    "             &(~y_2008_df.DepDelay.isnull())\\\n",
    "             &(~y_2008_df.DepDelayM.isnull())][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So columns DepDelay and DepDelayM are all identical (when DepDelay and DepDelayM are not null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ArrDelay\n",
    "#### Define\n",
    "- Calculate manually ArrDelay by subtracting ArrTime from CRSArrTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.ArrTime[0],y_2008_df.CRSArrTime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_between(y_2008_df.ArrTime[0],y_2008_df.CRSArrTime[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inspect the above result. Is the answer correct?\n",
    "- Now do this for all rows !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ArrDelays for all rows manually\n",
    "diff_list = []\n",
    "for val1,val2 in zip(y_2008_df.ArrTime,y_2008_df.CRSArrTime):\n",
    "    diff_list.append(days_between(val1,val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'ArrDelayM' and store the new calculated Delays\n",
    "y_2008_df['ArrDelayM']=diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ArrTime','CRSArrTime','ArrDelayM','ArrDelay']\n",
    "y_2008_df[cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mostly identical DepDelay and DepDelayM columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now what is the percentage of identical ArrDelayM and ArrDelay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.ArrDelay == y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())][cols].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.ArrDelay == y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())].shape[0], y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Around 95% is identical.(depends on random samples)\n",
    "- Therefore we still need to investigate the differences in columns between ArrDelay and ArrDelayM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the percentage of non-identical ArrDelayM and ArrDelay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.ArrDelay != y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())].shape[0], y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Around 1% (depends on random samples) where ArrDelayM and ArrDelay is not identical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.ArrDelay != y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())][cols].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looking at the table above, erros occur because ArrTime should happen the next early morning. Therefore just forward ArrTime by one day to fix this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- We need to forward ArrTime by one day. Then we need to re-calculate ArrDelayM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_forward_one = y_2008_df[(y_2008_df.ArrDelay != y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())][cols].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forwward ArrTime within index by oneday\n",
    "for indx in indx_forward_one:\n",
    "    y_2008_df.loc[indx,'ArrTime'] += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate Delay in mins\n",
    "for indx in indx_forward_one:\n",
    "    val1=y_2008_df.loc[indx,'ArrTime']\n",
    "    val2=y_2008_df.loc[indx,'CRSArrTime']\n",
    "    diff_min = days_between(val1,val2)\n",
    "    y_2008_df.loc[indx,'ArrDelayM'] = diff_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.ArrDelay != y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())].shape[0], y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.ArrDelay != y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())][cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So, I managed to reduce many rows. These rows just consist of minisclue of our data. Drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_to_del = y_2008_df[(y_2008_df.ArrDelay != y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())][cols].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.drop(index=indx_to_del,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df = y_2008_df.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.ArrDelay != y_2008_df.ArrDelayM)\\\n",
    "             &(~y_2008_df.ArrDelay.isnull())\\\n",
    "             &(~y_2008_df.ArrDelayM.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So columns ArrDelay and ArrDelayM are all identical (when ArrDelay and ArrDelayM are not null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Year,Month and DayofMonth and DayOfWeek\n",
    "y_2008_df.drop(columns=['Year','Month','DayofMonth','DayOfWeek'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop CRSDepTime, CRSArrTime\n",
    "#y_2008_df.drop(columns=['CRSDepTime','CRSArrTime',],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Adding Time Category\n",
    "#### Add Time Category according to  this list:\n",
    "- 24:00 (not 00:00) -- 02:59: Late Night <br>\n",
    "- 03:00 -- 05:59: Early Morning <br>\n",
    "- 06:00 -- 08:59: Morning <br>\n",
    "- 09:00 -- 11:59: Late Morning <br>\n",
    "- 12:00 -- 14:59: Afternoon <br>\n",
    "- 15:00 -- 17:59: Late Afternoon <br>\n",
    "- 18:00 -- 20:59: Evening <br>\n",
    "- 21:00 -- 23:59: Night <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour is a string from series\n",
    "def get_part_of_day(hour):\n",
    "    if pd.isnull(hour):\n",
    "        return np.NaN\n",
    "    else:\n",
    "        hour_temp = hour.strftime('%H')\n",
    "        if hour_temp == '00':\n",
    "            hour_int=int(0)\n",
    "        else:\n",
    "            hour_int = int(hour_temp)\n",
    "    \n",
    "    if 24 == hour_int:\n",
    "        return 'Late Night'\n",
    "    elif 0 <= hour_int <= 2:\n",
    "        return 'Late Night'\n",
    "    elif 3 <= hour_int <= 5:\n",
    "        return 'Early Morning'\n",
    "    elif 6 <= hour_int <= 8:\n",
    "        return 'Morning'\n",
    "    elif 9 <= hour_int <= 11:\n",
    "        return 'Late Morning'\n",
    "    elif 12 <= hour_int <= 14:\n",
    "        return 'Afternoon'\n",
    "    elif 15 <= hour_int <= 17:\n",
    "        return 'Late Afternoon'\n",
    "    elif 18 <= hour_int <= 20:\n",
    "        return 'Evening'\n",
    "    elif 21 <= hour_int <= 23:\n",
    "        return 'Night'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 DepTimeCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df['DepTimeCategory'] = y_2008_df.loc[: ,'DepTime'].apply(get_part_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.DepTimeCategory.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.DepTimeCategory.value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make DepTimeCategory into category with sorted order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_categories = ['Late Night','Early Morning','Morning','Late Morning',\n",
    "                 'Afternoon','Late Afternoon','Evening','Night']\n",
    "pd_ver = pd.__version__.split(\".\")\n",
    "if (int(pd_ver[0]) > 0) or (int(pd_ver[1]) >= 21): # v0.21 or later\n",
    "    day_classes = pd.api.types.CategoricalDtype(ordered = True, categories = day_categories)\n",
    "    y_2008_df['DepTimeCategory'] = y_2008_df['DepTimeCategory'].astype(day_classes)\n",
    "else: # pre-v0.21\n",
    "    y_2008_df['DepTimeCategory'] = y_2008_df['DepTimeCategory'].astype('category', ordered = True,\n",
    "                                                         categories = day_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.DepTimeCategory.value_counts(dropna=False,normalize=True).sort_index()\\\n",
    "            .plot(kind='barh',color=base_color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 ArrTimeCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df['ArrTimeCategory'] = y_2008_df.loc[: ,'ArrTime'].apply(get_part_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make ArrTimeCategory into category with sorted order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_categories = ['Late Night','Early Morning','Morning','Late Morning',\n",
    "                 'Afternoon','Late Afternoon','Evening','Night']\n",
    "pd_ver = pd.__version__.split(\".\")\n",
    "if (int(pd_ver[0]) > 0) or (int(pd_ver[1]) >= 21): # v0.21 or later\n",
    "    day_classes = pd.api.types.CategoricalDtype(ordered = True, categories = day_categories)\n",
    "    y_2008_df['ArrTimeCategory'] = y_2008_df['ArrTimeCategory'].astype(day_classes)\n",
    "else: # pre-v0.21\n",
    "    y_2008_df['ArrTimeCategory'] = y_2008_df['ArrTimeCategory'].astype('category', ordered = True,\n",
    "                                                         categories = day_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.ArrTimeCategory.value_counts(dropna=False).sort_index().plot(kind='barh',color=base_color);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.ArrTimeCategory.value_counts(dropna=False,normalize=True).sort_index()\\\n",
    "        .plot(kind='barh',color=base_color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exploring Cancelled -- Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Cancelled.value_counts(dropna=False,normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Cancelled.value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no null values.\n",
    "- Only 2% of flights are cancelled.\n",
    "**(Result is dependent on random sampling)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also notice that there are only two values 0s and 1s. Therefore convert Cancelled to bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing Cancelled type to boolean\n",
    "y_2008_df.Cancelled = y_2008_df.Cancelled.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_2008_df.Cancelled.value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CancellationCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CancellationCode.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancellation Code Legend\n",
    "> A: carrier <br>\n",
    "> B: weather <br>\n",
    "> C: NAS <br>\n",
    "> D: security <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled_true_cols = ['Cancelled','CancellationCode','DepTime','ArrTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[y_2008_df.Cancelled==True][cancelled_true_cols].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that when Cancelled is True, there is a corresponding reason for this cancellation <br>\n",
    "through CancellationCode column. The columns DepTime and ArrTime have null values. This makes <br>\n",
    "sense because flight is cancelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- When Cancelled==True, is there a corresponding CancellationCode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[y_2008_df.Cancelled==True][cancelled_true_cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CancellationCode.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As seen from above if you add total values of A,B,C and D, it tallies with total number of data when Cancelled==True. Therefore, we can conclude that when Cancelled==True, there is a corresponding CancellationCode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==True) & (y_2008_df.CancellationCode.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another test. If the answer is 0, then we can definitely conclude that when Cancelled==True, there is a corresponding CancellationCode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- When Cancelled==True, are all DepTime and ArrTime null?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==True) &\\\n",
    "          (~y_2008_df.DepTime.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==True) &\\\n",
    "          (~y_2008_df.DepTime.isnull())][cancelled_true_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are cases when DepTime is not null when Cancelled is True. Need to clean this data. Need to set DepTime to null when Cancelled is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==True) &\\\n",
    "          (~y_2008_df.ArrTime.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All ArrTime is null when Cancelled is True (for this random sample). But I will provide code to clean this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index that have problematic DepTime\n",
    "tofix_index = y_2008_df[(y_2008_df.Cancelled==True) &\\\n",
    "          (~y_2008_df.DepTime.isnull())].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When Cancelled is True, change DepTime to NaT (null)\n",
    "y_2008_df.loc[tofix_index,'DepTime'] = y_2008_df.loc[tofix_index,'DepTime'].apply(lambda x: pd.NaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tofix_index = y_2008_df[(y_2008_df.Cancelled==True) &\\\n",
    "          (~y_2008_df.ArrTime.isnull())].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tofix_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When Cancelled is True, change ArrTime to NaT (null)\n",
    "y_2008_df.loc[tofix_index,'ArrTime'] = y_2008_df.loc[tofix_index,'ArrTime'].apply(lambda x: pd.NaT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==True) &\\\n",
    "          (~y_2008_df.DepTime.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==True) &\\\n",
    "          (~y_2008_df.ArrTime.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When Cancelled==True, all DepTime and ArrTime is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Exploring CancellationCode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From previous discussion, we found out that when Cancelled==True, there is a corresponding CancellationCode. In this section we'll explore CancellationCode when Cancelled==False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (y_2008_df.CancellationCode.isnull())][cancelled_true_cols].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (y_2008_df.CancellationCode.isnull())].shape[0], y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.CancellationCode.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CancellationCode.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above data, when Cancellation is False, all CancellationCode is null. (Result dependent on random sampling). This is the result we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Exploring Diverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DepTime','ArrTime','Cancelled','CancellationCode','Diverted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Diverted == 1 and when Diverted == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[y_2008_df.Diverted==1][cols].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[y_2008_df.Diverted==0][cols].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data when diverted == 1\n",
    "y_2008_df[y_2008_df.Diverted==1][cols].shape[0],y_2008_df[y_2008_df.Diverted==0][cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Diverted.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Diverted.value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 99.8% is when Diverted is 0.\n",
    ">- Diverted==1 is 0.2%.\n",
    ">- There is no null values.\n",
    "> ##### (Result is dependent on random sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Diverted==1 and ArrTime is null / non-null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(y_2008_df.ArrTime.isnull())][cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(y_2008_df.ArrTime.isnull())][cols].shape[0],y_2008_df[y_2008_df.Diverted==1][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This situation (Diverted==1 and ArrTime is null) happens around 80% of the time. (Result is dependent on random sampling). This is a logical situation. When a plane is Diverted, there is no ArrTime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.ArrTime.isnull())][cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.ArrTime.isnull())][cols].shape[0], y_2008_df[y_2008_df.Diverted==1][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The proportion of non-null ArrTime given Diverted==1 is around 20%. (Result is dependent on random sampling). This is not logical. Therefore, I will clean this data. I Will make ArrTime to null when Diverted==1, which I will do below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- **Cleaning Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_to_clean = y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.ArrTime.isnull())][cols].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx in indx_to_clean:\n",
    "    y_2008_df.loc[indx,'ArrTime'] = pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Done!\n",
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.ArrTime.isnull())][cols].shape[0], y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(y_2008_df.ArrTime.isnull())][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Diverted==1 and DepTime is null / non-null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(y_2008_df.DepTime.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.DepTime.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.DepTime.isnull())][cols].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that when Diverted==1, all ArrTime is null and all DepTime is non-null. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Diverted==1 and DepTime non-null and Cancelled is True / False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.DepTime.isnull())\\\n",
    "          &(y_2008_df.Cancelled==False)][cols].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.DepTime.isnull())\\\n",
    "          &(y_2008_df.Cancelled==False)][cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.DepTime.isnull())\\\n",
    "          &(y_2008_df.Cancelled==True)][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Diverted==1)\\\n",
    "          &(~y_2008_df.DepTime.isnull())\\\n",
    "          &(y_2008_df.Cancelled==True)][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This above test yields an important insight! planes that are diverted are not cancelled! <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Exploring Cancelled -- Part Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DepTime','ArrTime','Cancelled','CancellationCode','Diverted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Cancelled==True, we expect all DepTime and ArrTime is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==True)\\\n",
    "          & (y_2008_df.DepTime.isnull())\n",
    "          & (y_2008_df.ArrTime.isnull())][cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==True)\\\n",
    "          & (y_2008_df.DepTime.isnull())\n",
    "          & (y_2008_df.ArrTime.isnull())][cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Cancelled.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the result above, we can be sure at this point that when Cancelled==True, DepTime and ArrTime is null. (for this data of random sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Cancelled==False, we expect DepTime and ArrTime is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.DepTime.isnull())\n",
    "          & (~y_2008_df.ArrTime.isnull())][cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.DepTime.isnull())\n",
    "          & (~y_2008_df.ArrTime.isnull())].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is what we expected. When Cancelled==False, DepTime and ArrTime is not null. This condition represent a bulk of our data, which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Cancelled==False and DepTime is null and ArrTime is not null\n",
    "- Rows with this condition shouldn't be in our data. Therefore, drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (y_2008_df.DepTime.isnull())\n",
    "          & (~y_2008_df.ArrTime.isnull())][cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (y_2008_df.DepTime.isnull())\n",
    "          & (~y_2008_df.ArrTime.isnull())][cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_to_drop = y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (y_2008_df.DepTime.isnull())\n",
    "          & (~y_2008_df.ArrTime.isnull())].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.drop(index=indx_to_drop,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (y_2008_df.DepTime.isnull())\n",
    "          & (~y_2008_df.ArrTime.isnull())][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Cancelled==False and DepTime is not null and ArrTime is null\n",
    "- ArrTime shouldn't be null if Diverted==0. If Diverted==1 and ArrTime is null, it is O.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.DepTime.isnull())\n",
    "          & (y_2008_df.ArrTime.isnull())\n",
    "          & (y_2008_df.Diverted==1)][cols].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.DepTime.isnull())\n",
    "          & (y_2008_df.ArrTime.isnull())\n",
    "          & (y_2008_df.Diverted==1)][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is O.K. ArrTime is null and Diverted==1. Because the plane is diverted, there is no information on ArrTime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.DepTime.isnull())\n",
    "          & (y_2008_df.ArrTime.isnull())\n",
    "          & (y_2008_df.Diverted==0)][cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.DepTime.isnull())\n",
    "          & (y_2008_df.ArrTime.isnull())\n",
    "          & (y_2008_df.Diverted==0)][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is not O.K. There is no Cancellation and no Diversion. ArrTime shouldn't be null. Drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_to_drop = y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.DepTime.isnull())\n",
    "          & (y_2008_df.ArrTime.isnull())\n",
    "          & (y_2008_df.Diverted==0)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.drop(index=indx_to_drop,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_2008_df[(y_2008_df.Cancelled==False)\\\n",
    "          & (~y_2008_df.DepTime.isnull())\n",
    "          & (y_2008_df.ArrTime.isnull())\n",
    "          & (y_2008_df.Diverted==0)][cols].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Exploring Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_delays = ['CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay',\\\n",
    "               'ArrDelayM','DepDelayM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[cols_delays].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sb.heatmap(y_2008_df[cols_delays].corr(),annot=True,cmap='rocket_r',fmt='.2f',vmin=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above heatmap we can see that:\n",
    "- a strong correlation between ArrDelayM and DepDelayM.\n",
    "- a strong correlation between CarrierDelay and ArrDelayM and DepDelayM.\n",
    "- a correlation between NASDelay and ArrDelayM.\n",
    "- a correlation between LateAircraftDelay and ArrDelayM and DepDelayM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when both ArrDelayM and DepDelayM is minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelayM<=0)\\\n",
    "         & (y_2008_df.ArrDelayM<=0)][cols_delays].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above table is logical, since CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay are null when there are no delays. There are no delays because ArrDelayM and DepDelayM are negative. \n",
    "- From my table, all  CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay columns are null. This depends on random sampling. Therefore it might be different for your dataset. Not all rows are null when there are no delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when both ArrDealyM and DepDelayM is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelayM>0)\\\n",
    "         & (y_2008_df.ArrDelayM>0)][cols_delays].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[(y_2008_df.DepDelay>0)\\\n",
    "         & (y_2008_df.ArrDelay>0)][cols_delays].shape[0] , y_2008_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above table, the columns CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay do not give rise to a logical ArrDelayM and DepDelayM.\n",
    "\n",
    "- This consists of about 30% of our data. If we drop these rows, we will drop many useful data.\n",
    "\n",
    "- Also, we need more insider knowledge if we want to re-calculate these rows into a logical ones.\n",
    "\n",
    "- Therefore instead of dropping these rows, I will drop columns CarrierDelay, WeatherDelay, NASDelay, SecurityDelay and LateAircraftDelay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping CarrierDelay, WeatherDelay, NASDelay, SecurityDelay and LateAircraftDelay columns.\n",
    "- Although there are some strong correlations between these columns and ArrDelayM and DepDelayM columns, we need to drop them by the reasonings we just gave above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_2008_df.drop(columns=['CarrierDelay','WeatherDelay','NASDelay','SecurityDelay',\\\n",
    "#                       'LateAircraftDelay'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Exploring Carriers table\n",
    "- we need to merge two tables: carriers_df and y_2008_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.UniqueCarrier.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The UniqueCarrier from y_2008_df is somehow a code for carriers. We need the description of the carrier to make the data intelligible to human readers. Can we get this description from carriers_df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_df.Code.isin(list(y_2008_df.UniqueCarrier)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The answer is Yes. We can find Carrier Description from carriers_df. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_df[carriers_df.Code.isin(list(y_2008_df.UniqueCarrier))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Therefore we need to merge carriers_df and y_2008_df. Before merging though, we need to do some data cleaning:\n",
    "\n",
    "- First, the Description from index 1308 above is a bit too long. We need to fix it.\n",
    "- Second, we need to change column names of carriers_df before merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Index 1308 of carriers_df has long Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(carriers_df.loc[1308].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the above description into 'US Airways Inc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_df.iloc[1308]['Description'] = 'US Airways Inc.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(carriers_df.loc[1308].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Before we can merge y_2008_df and carriers_df, we need to change the column names of carriers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_df.columns = ['UniqueCarrier','CarrierDescription']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Merge y_2008_df and carriers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df = pd.merge(carriers_df,y_2008_df,on='UniqueCarrier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[['UniqueCarrier','CarrierDescription']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carriers Distribution\n",
    "major airlines: https://en.wikipedia.org/wiki/Major_airlines_of_the_United_States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CarrierDescription.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.CarrierDescription.value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,7))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.barh(y_2008_df.CarrierDescription.value_counts().index,y_2008_df.CarrierDescription.value_counts())\n",
    "\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xticks(fontsize=13,rotation=45);\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'));\n",
    "plt.title('Carrier Distribution',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Exploring Airports table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations on airports_df:\n",
    "- iata is a unique id for an airport. The equivalent ids in y_2008_df is Origin and Dest columns. So if you want to merge these two data frames (airport_df and y_2008_df), merge on iata and Origin or iata and Dest.\n",
    "- The dataframe contains lat, long data. This pair of data will pinpoint the location of an airport on a map.\n",
    "- The dataframe also contains the airport's name, city state and country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Filter Airports that are located in the USA. Identify probelmatic data and then rectify  them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df[(airports_df.country.str.contains('USA')) & (airports_df.iata.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df[(airports_df.country.str.contains('USA')) & (airports_df.airport.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df[(airports_df.country.str.contains('USA')) & (airports_df.lat.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df[(airports_df.country.str.contains('USA')) & (airports_df.long.isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Columns iata, airport, country, lat and long have no null values. So all good for these columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df[(airports_df.country.str.contains('USA')) & (airports_df.city.isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The rows in table above have city and state null values. Fill them manually!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I google for the location of city and state of the above airports. Here is what I've found:<br><br>\n",
    "CLD: North San Diego County, California (CA) <br>\n",
    "HHH: Hilton Head Island, South Carolina (SC) <br>\n",
    "MIB: Minot, North Dakota (ND) <br>\n",
    "MQT: Marquette, Michigan (MI) <br>\n",
    "RCA: Rapid City, South Dakota (SD) <br>\n",
    "RDR: Grand Forks, North Dakota (ND) <br>\n",
    "SCE: Happy Valley, Pennsylvania (PA) <br>\n",
    "SKA: Spokane, Washington (WA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Fill null city and state values with the above data gathered from a search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_state_null_index = airports_df[airports_df.city.isnull() & airports_df.country.str.contains('USA')].index\n",
    "city_state_null_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = ['CA','SC','ND','MI','SD','ND','PA','WA']\n",
    "city_list = ['North San Diego County','Hilton Head Island','Minot','Marquette',\n",
    "             'Rapid City','Grand Forks','Happy Valley','Spokane']\n",
    "for i,indx in enumerate(city_state_null_index):\n",
    "    #print(str(i) + '--' + str(indx))\n",
    "    airports_df.loc[indx,'city'] = city_list[i]\n",
    "    airports_df.loc[indx,'state'] = state_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df[airports_df.city.isnull() & airports_df.country.str.contains('USA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.loc[city_state_null_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now the airports_df contains rows with filled data. Please note that this only applies to airports located in the USA.\n",
    "- Airports located in the USA are what we are interested in for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Remove rows that are not relevant to this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df[(airports_df.city.isnull()) | (airports_df.state.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_state_null_filter = (airports_df.city.isnull()) | (airports_df.state.isnull())\n",
    "city_state_null_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now only contains airport with city and state filled\n",
    "airports_df = airports_df[~city_state_null_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df[(airports_df.city.isnull()) | (airports_df.state.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Exploring y_2008_df and airports_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Origin\n",
    "- Origin Airport id which equals to iata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Origin','CarrierDescription','DepTime','DepTimeCategory','DepDelayM','Cancelled','Diverted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[cols].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above table is missing one column to be human-friendly. It is missing the Airport column. Now we have only airport code. Origin column contains airport code. We need to merge y_2008_df with airports_df to get this data along with lat and long data for airport's position on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Origin.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Origin.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no null values of Origin inside y_2008_df\n",
    "- There are around 290s unique Origin inside y_2008_df. (Result depends on random sampling)\n",
    "- Origin is airport_id or iata inside airports_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Are Origin inside y_2008_df are also represented inside airports_df?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of airports_df \n",
    "airports_origin_df = airports_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_origin_df.iata.nunique(), y_2008_df.Origin.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One important to note: `Origin` is an airport id. `iata` is also an airport id.\n",
    "- There are around 3,000s unique airport id inside airports_origin_df.\n",
    "- There are only around 290s unique airport id inside y_2008_df.\n",
    "- So make airport_origin_df to contain only airport id from y_2008_df. This I will do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_filter = airports_origin_df.iata.isin(list(y_2008_df.Origin))\n",
    "origin_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm only interested in the Origin in y_2008_df\n",
    "airports_origin_df = airports_origin_df[origin_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_origin_df.iata.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_origin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great! I now have a dataframe that contains detailed information about airports. Not just airport ids.\n",
    "- I will merge airports_origins_df and y_2008_df later. After merging, I will have detailed airport information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Dest\n",
    "- Destination Airport id which equals to iata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Dest','CarrierDescription','ArrTime','ArrTimeCategory','ArrDelayM','Cancelled','Diverted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[cols].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarly, the above above table is missing the Airport column. Now let's work towards that -- having detailed airport information in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Dest.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.Dest.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no null values of Dest inside y_2008_df\n",
    "- There are around 290s unique Dest inside y_2008_df. (Result depends on random sampling)\n",
    "- Dest is airport_id or iata inside airports_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Are Dest inside y_2008_df are also represented inside airports_df?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of airports_df \n",
    "airports_destination_df = airports_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_destination_df.iata.nunique(), y_2008_df.Dest.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One important to note: `Dest` is an airport id. `iata` is also an airport id.\n",
    "- There are around 3,000s unique airport id inside airports_destination_df.\n",
    "- There are only around 290s unique airport id inside y_2008_df.\n",
    "- So make airport_destination_df to contain only airport id from y_2008_df. This I will do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_filter = airports_destination_df.iata.isin(list(y_2008_df.Dest))\n",
    "dest_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm only interested in the Dest in y_2008_df\n",
    "airports_destination_df = airports_destination_df[dest_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_destination_df.iata.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_destination_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great! I now have a dataframe that contains detailed information about airports. Not just airport ids.\n",
    "- But there is an extra wrinkle. Is airports_origin_df and airports_destination_df identical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 airports_origin_df versus airports_destination_df identical?\n",
    "#### Define\n",
    "- find out if they are identical.\n",
    "- if not, make them identical and then which dataframe do you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- investigating airports_origin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~airports_origin_df.iata.isin(list(airports_destination_df.iata))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above result is the number of airports in airports_origin_df that is not in airports_destination_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_filter = airports_origin_df.iata.isin(list(airports_destination_df.iata))\n",
    "common_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~common_filter).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_origin_df[~common_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The airports above exist in airports_origin_df but NOT in airports_destination_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Investigating airports_destination_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~airports_destination_df.iata.isin(list(airports_origin_df.iata))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_filter = airports_destination_df.iata.isin(list(airports_origin_df.iata))\n",
    "common_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~common_filter).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_destination_df[~common_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The airports above exist in airports_destination_df but not in airports_origin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "- Clearly, airports_origin_df and airports_destination_df are almost identical except for the few rows.\n",
    "- Therefore, I will  remove a few rows from airports_origin_df and airports_destination_df. After which both tables will be identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_filter = airports_origin_df.iata.isin(list(airports_destination_df.iata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few rows removed\n",
    "airports_origin_df = airports_origin_df[common_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_filter = airports_destination_df.iata.isin(list(airports_origin_df.iata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few rows removed\n",
    "airports_destination_df = airports_destination_df[common_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~airports_destination_df.iata.isin(list(airports_origin_df.iata))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above result proves that airports_origin_df and airports_destination_df are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **To simplify though, I will arbitrariy choose airports_origin_df for this project. From now on, I will only mention/use airports_origin_df.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Merging y_2008_df and airports_origin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_origin_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To merge the above tables, we have to rename columns in airports_origin_df. \n",
    "- Rename iata to Origin.\n",
    "- Capitalize all column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing column names and capitalize first letter \n",
    "# Note that Origin == iata\n",
    "airports_origin_df.columns =['Origin','Airport','City','State','Country','Lat','Long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_origin_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = ['Origin','Airport','UniqueCarrier','CarrierDescription','City','State','Country',\n",
    "                'Lat','Long']\n",
    "airports_merged_df = pd.merge(airports_origin_df,y_2008_df,on='Origin')[desired_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The merged dataframe is called airports_merged_df.\n",
    "- The new dataframe's rows has increased from 290s rows to almost 70,000 rows.\n",
    "- With this new dataframe -- airports_merged_df -- I want to find out how many Carriers each airport is operating. Then I will categorize airport based on the number of Carriers it is operating.\n",
    "- Note the size of memory of airports_merged_df and y_2008_df. The former occupies 5.3 MB while the latter ocuupies 11.9 MB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_merged_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Airport Location and Categorizing Airport Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Airport Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_merged_df.Airport.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 290s unique airport in the USA based on our dataframe. (Result may differ due to random sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_merged_df[airports_merged_df.Airport.str.contains(\"Chicago O'Hare International\")]\\\n",
    "                    ['CarrierDescription'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_merged_df[airports_merged_df.Airport.str.contains(\"Chicago O'Hare International\")]\\\n",
    "                    ['CarrierDescription'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chicago O'Hare International operates around 14 Carriers in its airport. \n",
    "\n",
    "#### Define\n",
    "- I will iterate a list of airports and calculate how many Carriers each airport operates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_unique_list = airports_merged_df.Airport.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each airport, count carriers it carries.\n",
    "airport_carriers_count_dict = {}\n",
    "# airport_iata_dict={}\n",
    "for row in tqdm(airport_unique_list):\n",
    "    carriers_count = airports_merged_df[airports_merged_df.Airport.str.contains(row)]['CarrierDescription'].nunique()\n",
    "    try:\n",
    "        iata = airports_merged_df[airports_merged_df.Airport.str.contains(row)]['Origin'].values[0]\n",
    "        airport_carriers_count_dict[iata] = carriers_count\n",
    "    except:\n",
    "        # if there are errors , no element will be appended to airport_carriers_count_dict.\n",
    "        # it will print error\n",
    "        print('error at: ' + row)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue text string:** Merle K (Mudhole) Smith , Long Beach (Daugherty ) . use extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort\n",
    "airport_carriers_count = sorted(airport_carriers_count_dict.items(), key=operator.itemgetter(1))\n",
    "# now no longer a dict. now is a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 5 rows\n",
    "for row in airport_carriers_count[:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turned into dataframe\n",
    "raw_data = {'Origin': list(zip(*airport_carriers_count))[0], \n",
    "            'CarriersCount': list(zip(*airport_carriers_count))[1]}\n",
    "airport_count_carriers_df = pd.DataFrame(raw_data,columns=['Origin','CarriersCount'])\n",
    "airport_count_carriers_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df.columns=['iata','CarriersCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df = pd.merge(airport_count_carriers_df,airports_df,how='inner',on='iata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good! Now I have CarriersCount for each airport and location details!\n",
    "- It's time to visualize this data\n",
    "- Note: I have around 280s airport data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,25))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.barh(airport_count_carriers_df.airport,airport_count_carriers_df.CarriersCount);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have to many airport rows -- around 280s rows.\n",
    "- Therefore need alternative plots other than barh to visualize data.\n",
    "- First, I will use scatter plot. \n",
    "- Then I will use plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.scatter(airport_count_carriers_df.airport,airport_count_carriers_df.CarriersCount)\n",
    "plt.xticks('');\n",
    "plt.ylabel('Carriers Count\\n',fontsize=13)\n",
    "plt.xlabel('Airports',fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enter Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use plotly to visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df['text'] = airport_count_carriers_df['airport'] + '' + airport_count_carriers_df['city'] +\\\n",
    "    ', ' + airport_count_carriers_df['state'] + '' + 'Count: ' + airport_count_carriers_df['CarriersCount'].astype(str)\n",
    "\n",
    "scl = [ [0,\"rgb(5, 10, 172)\"],[0.35,\"rgb(40, 60, 190)\"],[0.5,\"rgb(70, 100, 245)\"],\\\n",
    "    [0.6,\"rgb(90, 120, 245)\"],[0.7,\"rgb(106, 137, 247)\"],[1,\"rgb(220, 220, 220)\"] ]\n",
    "\n",
    "data = [ dict(\n",
    "        type = 'scattergeo',\n",
    "        locationmode = 'USA-states',\n",
    "        lon = airport_count_carriers_df['long'],\n",
    "        lat = airport_count_carriers_df['lat'],\n",
    "        text = airport_count_carriers_df['text'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            opacity = 0.8,\n",
    "            reversescale = True,\n",
    "            autocolorscale = False,\n",
    "            symbol = 'round',\n",
    "            line = dict(\n",
    "                width=1,\n",
    "                color='rgba(102, 102, 102)'\n",
    "            ),\n",
    "            colorscale = scl,\n",
    "            cmin = 1,\n",
    "            color = airport_count_carriers_df['CarriersCount'],\n",
    "            cmax = airport_count_carriers_df['CarriersCount'].max(),\n",
    "            colorbar=dict(\n",
    "                title=\"Number of Airline Carriers\"\n",
    "            )\n",
    "        ))]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Number of Airline Carriers Operating in an Airport Origin<br>(Hover for airport names)',\n",
    "        colorbar = True,\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showland = True,\n",
    "            landcolor = \"rgb(250, 250, 250)\",\n",
    "            subunitcolor = \"rgb(217, 217, 217)\",\n",
    "            countrycolor = \"rgb(217, 217, 217)\",\n",
    "            countrywidth = 0.5,\n",
    "            subunitwidth = 0.5\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "py.iplot( fig, validate=False, filename='andy-airports-origin' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the map above, most airports cluster on the East coast. There are also many airports in Alaska and Hawaii states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another look with cmin=10. Airports with at least 10 airline carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df['text'] = airport_count_carriers_df['airport'] + '' + airport_count_carriers_df['city'] +\\\n",
    "    ', ' + airport_count_carriers_df['state'] + '' + 'Count: ' + airport_count_carriers_df['CarriersCount'].astype(str)\n",
    "\n",
    "scl = [ [0,\"rgb(5, 10, 172)\"],[0.35,\"rgb(40, 60, 190)\"],[0.5,\"rgb(70, 100, 245)\"],\\\n",
    "    [0.6,\"rgb(90, 120, 245)\"],[0.7,\"rgb(106, 137, 247)\"],[1,\"rgb(220, 220, 220)\"] ]\n",
    "\n",
    "data = [ dict(\n",
    "        type = 'scattergeo',\n",
    "        locationmode = 'USA-states',\n",
    "        lon = airport_count_carriers_df['long'],\n",
    "        lat = airport_count_carriers_df['lat'],\n",
    "        text = airport_count_carriers_df['text'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            opacity = 0.8,\n",
    "            reversescale = True,\n",
    "            autocolorscale = False,\n",
    "            symbol = 'round',\n",
    "            line = dict(\n",
    "                width=1,\n",
    "                color='rgba(102, 102, 102)'\n",
    "            ),\n",
    "            colorscale = scl,\n",
    "            cmin = 10,\n",
    "            color = airport_count_carriers_df['CarriersCount'],\n",
    "            cmax = airport_count_carriers_df['CarriersCount'].max(),\n",
    "            colorbar=dict(\n",
    "                title=\"Numbe of Airline Carriers\"\n",
    "            )\n",
    "        ))]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Number of Airline Carriers Operating in an Airport Origin<br>cmin=10 -- lowest airport is 10<br>(Hover for airport names)',\n",
    "        colorbar = True,\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showland = True,\n",
    "            landcolor = \"rgb(250, 250, 250)\",\n",
    "            subunitcolor = \"rgb(217, 217, 217)\",\n",
    "            countrycolor = \"rgb(217, 217, 217)\",\n",
    "            countrywidth = 0.5,\n",
    "            subunitwidth = 0.5\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "py.iplot( fig, validate=False, filename='andy-airports2-origin' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With cmin=10, airports with at least 10 carriers tend to concentrate along coast lines, mid-west and Texas. Investigate the airports in the central areas such as ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Categorizing Airport Size\n",
    "#### Define\n",
    "- Now that we have information about how many Carriers each Airport operates, it is time to categorize Airport size based on how many Carriers it operates.\n",
    "- First, merge airport_count_carriers_df and y_2008_df. I want the CarriersCount data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_desired = list(y_2008_df.columns)\n",
    "columns_desired.append('CarriersCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df.columns = ['Origin','CarriersCount','airport','city','state','country'\\\n",
    "                         ,'lat','long','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge airport_count_carriers_df and y_2008_df\n",
    "y_2008_df = pd.merge(y_2008_df,airport_count_carriers_df,on='Origin',how='inner')[columns_desired]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have CarriersCount data in y_2008_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "- Not it's time to categorize Airport size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_count_carriers_df.CarriersCount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_25th = np.percentile(airport_count_carriers_df.CarriersCount,25)  # 25 percentilenp\n",
    "percentile_50th = np.percentile(airport_count_carriers_df.CarriersCount,50)  # 50th percentile\n",
    "percentile_75th = np.percentile(airport_count_carriers_df.CarriersCount,75)  # 75 percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_airport_sized(series):\n",
    "    #print(series)\n",
    "    row = series\n",
    "    #if row <= 1:\n",
    "    if row <= percentile_25th:\n",
    "        #print('v_small_sized')\n",
    "        return 'v_small_sized'\n",
    "        #y_2008_df.loc[indx,'airport_sized'] = 'v_small_sized'\n",
    "    elif percentile_25th < row <= percentile_50th:\n",
    "    #elif 1 < row <= 10:\n",
    "        #print('small_sized')\n",
    "        return 'small_sized'\n",
    "        #y_2008_df.loc[indx,'airport_sized'] = 'small_sized'\n",
    "    elif percentile_50th < row <= percentile_75th:\n",
    "    #elif 10 < row <= 14:\n",
    "        #print('mid_sized')\n",
    "        return 'mid_sized'\n",
    "        #y_2008_df.loc[indx,'airport_sized'] = 'mid_sized'\n",
    "    elif percentile_75th < row:\n",
    "    #elif 14 < row <= 20:\n",
    "        #print('big_sized')\n",
    "        return 'big_sized'\n",
    "        #y_2008_df.loc[indx,'airport_sized'] = 'big_sized'\n",
    "    else:\n",
    "        #print('eroor')\n",
    "        return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_2008_df['airport_sized'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df['airport_sized'] = y_2008_df.CarriersCount.apply(allocate_airport_sized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.airport_sized.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make `airport_sized` into category with sorted order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_size_categories = ['v_small_sized','small_sized','mid_sized','big_sized']\n",
    "pd_ver = pd.__version__.split(\".\")\n",
    "if (int(pd_ver[0]) > 0) or (int(pd_ver[1]) >= 21): # v0.21 or later\n",
    "    airport_classes = pd.api.types.CategoricalDtype(ordered = True, categories = airport_size_categories)\n",
    "    y_2008_df['airport_sized'] = y_2008_df['airport_sized'].astype(airport_classes)\n",
    "else: # pre-v0.21\n",
    "    y_2008_df['airport_sized'] = y_2008_df['airport_sized'].astype('category', ordered = True,\n",
    "                                                         categories = airport_size_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.airport_sized.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- airport_sized is a category type and sorted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Exploring FlightNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.FlightNum.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No null values or very few null values for FlightNum. Seems promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.groupby(['FlightNum','CarrierDescription']).count().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I expect FlightNum to be unique for each airlines/carriers. But it is not. So I don't find this column useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop FlightNum column. (haven't decided yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_2008_df.drop(columns=['FlightNum'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Exploring TailNum\n",
    "#### <a href='https://en.wikipedia.org/wiki/Aircraft_registration'>From Wikipedia</a>: \n",
    "\n",
    "- Every civil aircraft must be marked prominently on its exterior by an alphanumeric string, indicating its country of registration and its unique serial number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.groupby(['TailNum','CarrierDescription']).count().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.TailNum.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This column seems like a promising column for analysis. For example, I can turn this into ordinal categories: Boeing 747, 737, Airbus ... And explore whether delays have to do with size and make of an aircraft. But due to time constraint, I will not explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop TailNum column (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_2008_df.drop(columns=['TailNum'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Exploring Actual/CRS ElapsedTime, AirTime, Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = ['ActualElapsedTime','AirTime','ArrDelayM','DepDelayM','Distance','CarriersCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desired_cols = ['ActualElapsedTime','AirTime','ArrDelayM','DepDelayM','Distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.PairGrid(data=y_2008_df,vars=desired_cols)\n",
    "#g = g.map_diag(plt.hist,bins=20)\n",
    "g.map(plt.scatter);\n",
    "#g.map_offdiag(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df[desired_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(y_2008_df[desired_cols].corr(),annot=True,cmap='rocket_r',fmt='.2f',vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Exploring TaxiIn and TaxiOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sb.boxplot(data=y_2008_df,y=y_2008_df.DepDelayM,x=y_2008_df.airport_sized,hue='DepTimeCategory'\\\n",
    "          ,showfliers=False,palette='coolwarm')\n",
    "plt.ylim(-30,230);\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot title here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sb.boxplot(data=y_2008_df,y=y_2008_df.ArrDelayM,x=y_2008_df.airport_sized,hue='ArrTimeCategory'\\\n",
    "          ,showfliers=False,palette='coolwarm')\n",
    "plt.ylim(-60,230)\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot title here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sb.pointplot(data=y_2008_df,y=y_2008_df.DepDelayM,x=y_2008_df.airport_sized,hue='DepTimeCategory',ci=None,\n",
    "            linestyles='',dodge=0.3,)\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot title there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sb.pointplot(data=y_2008_df,y=y_2008_df.DepDelay,x=y_2008_df.airport_sized,hue='DepTimeCategory',ci=None,\n",
    "            linestyles='',dodge=0.3)\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sb.barplot(data=y_2008_df,y=y_2008_df.DepDelay,x=y_2008_df.airport_sized,hue='DepTimeCategory',ci='sd')\n",
    "plt.legend(fontsize=14);\n",
    "plt.ylim(-60,200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = y_2008_df[(~y_2008_df.DepDelayM.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_data_df= plot_data[['CarrierDescription','DepDelayM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_data_df.groupby('CarrierDescription').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_data = y_2008_df[(~y_2008_df.DepDelayM.isnull()) & (y_2008_df.DepTimeCategory!='Late Night')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data.groupby('CarrierDescription')['DepDelayM'].mean().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_index = plot_data.groupby('CarrierDescription')['DepDelayM'].mean().sort_values().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider putting the number of flights each carriers flies as hue or size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "sb.pointplot(data=plot_data,y='CarrierDescription',x='DepDelayM',linestyles='',ci=None,order=order_index)\n",
    "plt.yticks(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Savings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_2008_full_df = pd.read_csv('../data_flight/2008/2008.csv')\n",
    "#carriers_df = pd.read_csv('../data_flight/supplement/carriers.csv')\n",
    "#airports_df = pd.read_csv('../data_flight/supplement/airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2008_df.to_csv('../y_2008_all_data',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
